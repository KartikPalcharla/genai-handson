{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe75baa",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80a8ddf",
   "metadata": {},
   "source": [
    "# Unit 2 - Part 3a: Chain of Thought (CoT)\n",
    "\n",
    "## 1. Introduction: The Inner Monologue\n",
    "\n",
    "Standard LLMs try to jump straight to the answer. For complex problems (math, logic), this often fails.\n",
    "\n",
    "**Chain of Thought (CoT)** forces the model to \"think out loud\" before answering. \n",
    "\n",
    "### Why use a \"Dumb\" Model?\n",
    "For this unit, we will use **Llama3.1-8b** (via Groq). It is a smaller, faster model.\n",
    "Why? Because huge models (like Gemini Pro or GPT-4) are often *too smart*—they solve logic riddles instantly without thinking.\n",
    "\n",
    "To really see the power of Prompt Engineering, we need a model that **needs help**.\n",
    "\n",
    "### Visualizing the Process (Flowchart)\n",
    "```mermaid\n",
    "graph TD\n",
    "    Input[Question: 5+5*2?]\n",
    "    Input -->|Standard| Wrong[Answer: 20 (Wrong)]\n",
    "    Input -->|CoT| Step1[Step 1: 5*2=10]\n",
    "    Step1 --> Step2[Step 2: 5+10=15]\n",
    "    Step2 --> Correct[Answer: 15 (Correct)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a41e5ba",
   "metadata": {},
   "source": [
    "## 2. Concept: Latent Reasoning\n",
    "\n",
    "Why does this work?\n",
    "Because LLMs are \"Next Token Predictors\".\n",
    "- If you force it to answer immediately, it must predict the digits `1` and `5` immediately.\n",
    "- If you let it \"think\", it generates intermediate tokens (`5`, `*`, `2`, `=`, `1`, `0`).\n",
    "- The model then **ATTENDS** to these new tokens to compute the final answer.\n",
    "\n",
    "**Writing is Thinking.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba92b198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
    "\n",
    "# Using Llama3.1-8b (Small/Fast) to demonstrate logic failures\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3088780",
   "metadata": {},
   "source": [
    "## 3. The Experiment: A Tricky Math Problem\n",
    "\n",
    "Let's try a problem that requires multi-step logic.\n",
    "\n",
    "**Problem:**\n",
    "\"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many does he have now?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a70d3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STANDARD (Llama3.1-8b) ---\n",
      "To find out how many tennis balls Roger has now, we need to add the initial number of tennis balls he had (5) to the number of tennis balls he bought (2 cans * 3 tennis balls per can).\n",
      "\n",
      "2 cans * 3 tennis balls per can = 6 tennis balls\n",
      "\n",
      "Now, let's add the initial number of tennis balls (5) to the number of tennis balls he bought (6):\n",
      "\n",
      "5 + 6 = 11\n",
      "\n",
      "So, Roger now has 11 tennis balls.\n"
     ]
    }
   ],
   "source": [
    "question = \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many does he have now?\"\n",
    "\n",
    "# 1. Standard Prompt (Direct Answer)\n",
    "prompt_standard = f\"Answer this question: {question}\"\n",
    "print(\"--- STANDARD (Llama3.1-8b) ---\")\n",
    "print(llm.invoke(prompt_standard).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b696ba6",
   "metadata": {},
   "source": [
    "### Critique\n",
    "Smaller models often latch onto the visible numbers (5 and 2) and simply add them (7), ignoring the multiplication step implied by \"cans\".\n",
    "\n",
    "Let's force it to think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd65b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chain of Thought (Llama3.1-8b) ---\n",
      "To find out how many tennis balls Roger has now, we need to follow these steps:\n",
      "\n",
      "1. Roger already has 5 tennis balls.\n",
      "2. He buys 2 more cans of tennis balls. Each can has 3 tennis balls, so he buys 2 x 3 = 6 more tennis balls.\n",
      "3. Now, we add the tennis balls he already had (5) to the tennis balls he bought (6). 5 + 6 = 11\n",
      "\n",
      "So, Roger now has 11 tennis balls.\n"
     ]
    }
   ],
   "source": [
    "# 2. CoT Prompt (Magic Phrase)\n",
    "prompt_cot = f\"Answer this question. Let's think step by step. {question}\"\n",
    "\n",
    "print(\"--- Chain of Thought (Llama3.1-8b) ---\")\n",
    "print(llm.invoke(prompt_cot).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd205672",
   "metadata": {},
   "source": [
    "## 4. Analysis\n",
    "\n",
    "Look at the output. By explicitly breaking it down:\n",
    "1.  \"Roger starts with 5.\"\n",
    "2.  \"2 cans * 3 balls = 6 balls.\"\n",
    "3.  \"5 + 6 = 11.\"\n",
    "\n",
    "The model effectively \"debugs\" its own logic by generating the intermediate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ee779f",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d1fa7c",
   "metadata": {},
   "source": [
    "# Unit 2 - Part 3b: Tree of Thoughts (ToT) & Graph of Thoughts (GoT)\n",
    "\n",
    "## 1. Introduction: Beyond A -> B\n",
    "\n",
    "CoT is linear. But complex reasoning is often nonlinear. We need to explore branches (ToT) or even combine ideas (GoT).\n",
    "\n",
    "We continue using **Llama3.1-8b via Groq** to show how structure improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4371aa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
    "\n",
    "# Using Llama3.1-8b\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7) # Creativity needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a348d6",
   "metadata": {},
   "source": [
    "## 2. Tree of Thoughts (ToT)\n",
    "\n",
    "ToT explores multiple branches before making a decision. \n",
    "**Analogy:** A chess player considering 3 possible moves before playing one.\n",
    "\n",
    "### Implementation\n",
    "We will generate 3 distinct solutions for a problem and then use a \"Judge\" to pick the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ea2d4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tree of Thoughts (ToT) Result ---\n",
      "As a child psychologist, I would recommend **Solution 3: Create a \"Munchkin Garden\"** as the most sustainable approach to encouraging a 5-year-old to eat vegetables. This solution is not bribery, as it doesn't offer a reward or punishment, but rather involves the child in the process of planning and creating their meal. Here's why I believe this approach is sustainable:\n",
      "\n",
      "1. **Involvement and agency**: By involving the child in meal planning, grocery shopping, and vegetable preparation, they feel more in control and responsible for their meal. This sense of agency can lead to increased motivation to try new foods.\n",
      "2. **Creativity and imagination**: The Munchkin Garden approach encourages the child to think creatively and use their imagination to design their meal. This can make mealtime feel more engaging and fun, rather than a chore.\n",
      "3. **Positive relationships**: By making mealtime a collaborative experience, the child develops a positive relationship with food and the people involved in preparing it. This can lead to a more positive attitude towards trying new foods.\n",
      "4. **Developing skills**: The Munchkin Garden approach can help the child develop important skills, such as planning, problem-solving, and decision-making. These skills can be transferable to other areas of life, such as academics and social relationships.\n",
      "5. **Long-term effects**: By involving the child in the process of creating their meal, they are more likely to develop a long-term interest in trying new foods and exploring different cuisines.\n",
      "6. **Parent-child bonding**: The Munchkin Garden approach can strengthen the parent-child bond, as they work together to plan and prepare meals. This can lead to a more positive and supportive relationship.\n",
      "\n",
      "While the other solutions, such as creating a \"Vegetable Face\" or \"Veggie Detective\" challenge, can be fun and engaging, they may not be as sustainable in the long-term. The \"Vegetable Face\" approach may become repetitive and less engaging as the child gets older, while the \"Veggie Detective\" challenge may require more effort and resources to maintain. The Munchkin Garden approach, on the other hand, can be adapted to suit the child's interests and developmental stage, making it a more sustainable solution.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "problem = \"How can I get my 5-year-old to eat vegetables?\"\n",
    "\n",
    "# Step 1: The Branch Generator\n",
    "prompt_branch = ChatPromptTemplate.from_template(\n",
    "    \"Problem: {problem}. Give me one unique, creative solution. Solution {id}:\"\n",
    ")\n",
    "\n",
    "branches = RunnableParallel(\n",
    "    sol1=prompt_branch.partial(id=\"1\") | llm | StrOutputParser(),\n",
    "    sol2=prompt_branch.partial(id=\"2\") | llm | StrOutputParser(),\n",
    "    sol3=prompt_branch.partial(id=\"3\") | llm | StrOutputParser(),\n",
    ")\n",
    "\n",
    "# Step 2: The Judge\n",
    "prompt_judge = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    I have three proposed solutions for: '{problem}'\n",
    "    \n",
    "    1: {sol1}\n",
    "    2: {sol2}\n",
    "    3: {sol3}\n",
    "    \n",
    "    Act as a Child Psychologist. Pick the most sustainable one (not bribery) and explain why.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Chain: Input -> Branches -> Judge -> Output\n",
    "tot_chain = (\n",
    "    RunnableParallel(problem=RunnableLambda(lambda x: x), branches=branches)\n",
    "    | (lambda x: {**x[\"branches\"], \"problem\": x[\"problem\"]}) \n",
    "    | prompt_judge\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"--- Tree of Thoughts (ToT) Result ---\")\n",
    "print(tot_chain.invoke(problem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38579cab",
   "metadata": {},
   "source": [
    "## 3. Graph of Thoughts (GoT)\n",
    "\n",
    "You asked: **\"Where is Graph of Thoughts?\"**\n",
    "\n",
    "GoT is more complex. It's a network. Information can split, process specific parts, and then **AGGREGATE** back together.\n",
    "\n",
    "### The Workflow (Writer's Room)\n",
    "1.  **Split:** Generate 3 independent story plots (Sci-Fi, Fantasy, Mystery).\n",
    "2.  **Aggregate:** The model reads all 3 and creates a \"Master Plot\" that combines the best elements of each.\n",
    "3.  **Refine:** Polish the Master Plot.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "   Start(Concept) --> A[Draft 1]\n",
    "   Start --> B[Draft 2]\n",
    "   Start --> C[Draft 3]\n",
    "   A & B & C --> Mixer[Aggregator]\n",
    "   Mixer --> Final[Final Story]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "894940b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Graph of Thoughts (GoT) Result ---\n",
      "\"Chrono Eclipse\" is a thrilling, heart-pounding, and mind-bending adventure that brings together the best of science fiction, romance, and horror. In a world where time manipulation has become a reality, a brilliant physicist, James, discovers a way to traverse the timestream, but his experiment goes catastrophically wrong, unleashing a malevolent entity from the past - a vengeful spirit of a woman named Sophia, who was lost in the very same timestream that James manipulated. As James navigates through the complex web of timelines to prevent the catastrophic mistake that threatens the fabric of existence, he finds himself falling for a woman named Emily, who bears an uncanny resemblance to Sophia. But every time James tries to repair the timeline, Sophia's malevolent presence grows stronger, forcing Emily to confront the dark side of her own past and the terrifying truth that she may be the key to unlocking the secrets of the timestream, and that her love for James may be the very thing that destroys them all.\n"
     ]
    }
   ],
   "source": [
    "# 1. The Generator (Divergence)\n",
    "prompt_draft = ChatPromptTemplate.from_template(\n",
    "    \"Write a 1-sentence movie plot about: {topic}. Genre: {genre}.\"\n",
    ")\n",
    "\n",
    "drafts = RunnableParallel(\n",
    "    draft_scifi=prompt_draft.partial(genre=\"Sci-Fi\") | llm | StrOutputParser(),\n",
    "    draft_romance=prompt_draft.partial(genre=\"Romance\") | llm | StrOutputParser(),\n",
    "    draft_horror=prompt_draft.partial(genre=\"Horror\") | llm | StrOutputParser(),\n",
    ")\n",
    "\n",
    "# 2. The Aggregator (Convergence)\n",
    "prompt_combine = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    I have three movie ideas for the topic '{topic}':\n",
    "    1. Sci-Fi: {draft_scifi}\n",
    "    2. Romance: {draft_romance}\n",
    "    3. Horror: {draft_horror}\n",
    "    \n",
    "    Your task: Create a new Mega-Movie that combines the TECHNOLOGY of Sci-Fi, the PASSION of Romance, and the FEAR of Horror.\n",
    "    Write one paragraph.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# 3. The Chain\n",
    "got_chain = (\n",
    "    RunnableParallel(topic=RunnableLambda(lambda x: x), drafts=drafts)\n",
    "    | (lambda x: {**x[\"drafts\"], \"topic\": x[\"topic\"]}) \n",
    "    | prompt_combine\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"--- Graph of Thoughts (GoT) Result ---\")\n",
    "print(got_chain.invoke(\"Time Travel\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455cb724",
   "metadata": {},
   "source": [
    "## 4. Summary & Comparison Table\n",
    "\n",
    "| Method | Structure | Best For... | Cost/Latency |\n",
    "|--------|-----------|-------------|--------------|\n",
    "| **Simple Prompt** | Input -> Output | Simple facts, summaries | ⭐ Low |\n",
    "| **CoT (Chain)** | Input -> Steps -> Output | Math, Logic, Debugging | ⭐⭐ Med |\n",
    "| **ToT (Tree)** | Input -> 3x Branches -> Select -> Output | Strategic decisions, Brainstorming | ⭐⭐⭐ High | \n",
    "| **GoT (Graph)** | Input -> Branch -> Mix/Aggregate -> Output | Creative Writing, Research Synthesis | ⭐⭐⭐⭐ V. High |\n",
    "\n",
    "**Recommendation:** Start with CoT. Only use ToT/GoT if CoT fails."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
